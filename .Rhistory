library(dplyr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(rvest)
library(XML)
library(stringr)
library(ggthemes)
library(pdftools)
library(tm
)
setwd("~/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs")
setwd("~/Documents/GitHub/Tax-Fraud-and-NLP-Demo")
directory <- getwd("~/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs") # change this to directory where files are located
directory <- getwd("~/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs")
getwd()
directory <- getwd("/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs") # change this to directory where files are located
directory
getwd()
directory <- setwd("/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs") # change this to directory where files are located
pdf_corpus <- VCorpus(DirSource(directory, pattern = ".pdf"),
readerControl = list(reader = readPDF))
directory <- getwd("/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs") # change this to directory where files are located
directory <- "/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs" # change this to directory where files are located
pdf_corpus <- VCorpus(DirSource(directory, pattern = ".pdf"),
readerControl = list(reader = readPDF))
View(pdf_corpus)
as.character(pdf_corpus)
new_corpus <- as.character(pdf_corpus)
View(new_corpus)
new_corpus <- as.data.frame(pdf_corpus)
pdf_corpus
summary(pdf_corpus)
View(pdf_corpus)
dest <- "/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs"          #same as setwd()
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
lapply(myfiles, function(i) system(paste('"/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs',    #the path to Program files where the pdftotext.exe is saved
paste0('"', i, '"')), wait = FALSE) )
files <- list.files(pattern = "[.]txt$")
outFile <- file("output.txt", "w")
for (i in files){
x <- readLines(i)
writeLines(x[2:(length(x)-1)], outFile)
}
close(outFile)
files
lapply(myfiles, function(i) system(paste('/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs',    #the path to Program files where the pdftotext.exe is saved
paste0('"', i, '"')), wait = FALSE) )
lapply(myfiles, function(i) system(paste('/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs',    #the path to Program files where the pdftotext.exe is saved
paste0('/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs', i, '"')), wait = FALSE) )
lapply(myfiles, function(i) system(paste('""',    #the path to Program files where the pdftotext.exe is saved
paste0('"', i, '"')), wait = FALSE) )
getwd()
directory <- "/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs"
file.list <- paste(directory, "/",list.files(directory, pattern = "*.pdf"), sep = "")
lapply(file.list, FUN = function(files) {
pdf_convert(files, format = "txt")
})
lapply(file.list, FUN = function(files) {
pdf_convert(files, format = ".txt")
})
all_pdfs <- list.files(pattern = ".pdf$")
all_pdfs <- list.files(pattern = ".pdf$")
View(all_pdfs)
all_pdfs
library(tidytext)
map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
install.packages("pdftools")
install.packages("pdftools")
library(pdftools)
map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
?map_df
??map_df
df <- map_df(all_pdfs[3:5], ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = basename(.x)) %>%
unnest_tokens(word, txt)
)
library(purrr)
map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
library(dplyr)
all_pdfs <- list.files(pattern = ".pdf$")
map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(rvest)
library(XML)
library(stringr)
library(ggthemes)
library(pdftools)
library(tm)
library(tidytext)
library(purrr)
map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
write.csv(Corpus,'policydocs.csv')
write.csv(all_pdfs,'policydocs.csv')
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x)
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x))
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x)
all_pdfs <- list.files(pattern = ".pdf$")
directory <- "/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs"
all_pdfs <- list.files(pattern = ".pdf$")
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x)
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x)
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x)
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x))
all_pdfs <- list.files(pattern = ".pdf$")
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x))
)
View(Corpus)
Corpus %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
Corpus %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt)
write.csv(Corpus,'policydocs.csv')
write.csv(Corpus,'policydocs.csv')
View(wiki_urls)
html_urls <- c('https://en.wikipedia.org/wiki/Tax_avoidance',
'https://en.wikipedia.org/wiki/Tax_shelter',
'https://en.wikipedia.org/wiki/Tax_haven',
'https://en.wikipedia.org/wiki/Offshore_financial_centre',
'https://en.wikipedia.org/wiki/Tax_residence',
'https://en.wikipedia.org/wiki/Panama_Papers',
'https://en.wikipedia.org/wiki/Money_laundering',
'https://en.wikipedia.org/wiki/Suspicious_activity_report',
'https://en.wikipedia.org/wiki/Financial_Action_Task_Force_on_Money_Laundering',
'https://en.wikipedia.org/wiki/Bank_of_Credit_and_Commerce_International',
'https://en.wikipedia.org/wiki/Paradise_Papers',
'https://en.wikipedia.org/wiki/Bahamas_Leaks',
'https://en.wikipedia.org/wiki/Operation_Car_Wash')
wiki_urls <- lapply(html_urls, function(x) read_html(x) %>% html_text())
evasion <- do.call(rbind, Map(data.frame, text= wiki_urls))
View(evasion)
write.csv(evasion,'wikidocs.csv')
setwd("~/Documents/GitHub/Tax-Fraud-and-NLP-Demo")
evasion$text <- as.character(evasion$text)
View(evasion$text)
evasion_words %>%
count(word, sort=TRUE)
evasion_words %>%
count(word, sort = TRUE) %>%
filter(n > 200) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + ggtitle("The Most Common Words in Corpus") + theme_minimal()
evasion <- do.call(rbind, Map(data.frame, text= wiki_urls))
evasion_words <- evasion %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
evasion_words <- evasion %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
html_urls <- c('https://en.wikipedia.org/wiki/Tax_avoidance',
'https://en.wikipedia.org/wiki/Tax_shelter',
'https://en.wikipedia.org/wiki/Tax_haven',
'https://en.wikipedia.org/wiki/Offshore_financial_centre',
'https://en.wikipedia.org/wiki/Tax_residence',
'https://en.wikipedia.org/wiki/Panama_Papers',
'https://en.wikipedia.org/wiki/Money_laundering',
'https://en.wikipedia.org/wiki/Suspicious_activity_report',
'https://en.wikipedia.org/wiki/Financial_Action_Task_Force_on_Money_Laundering',
'https://en.wikipedia.org/wiki/Bank_of_Credit_and_Commerce_International',
'https://en.wikipedia.org/wiki/Paradise_Papers',
'https://en.wikipedia.org/wiki/Bahamas_Leaks',
'https://en.wikipedia.org/wiki/Operation_Car_Wash')
wiki_urls <- lapply(html_urls, function(x) read_html(x) %>% html_text())
evasion <- do.call(rbind, Map(data.frame, text= wiki_urls))
evasion_words <- evasion %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
evasion <- do.call(rbind, Map(data.frame, text= wiki_urls))
evasion_words <- evasion %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
evasion_words %>%
count(word, sort=TRUE)
evasion_words <- evasion %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
evasion$text <- as.character(evasion$text)
evasion_words <- evasion %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
evasion_words %>%
count(word, sort=TRUE)
evasion_words %>%
count(word, sort=TRUE) %>%
filter(n > 50)
top_50 <- evasion_words %>%
count(word, sort=TRUE) %>%
filter(n > 50)
View(top_50)
top_words <- evasion_words %>%
count(word, sort=TRUE) %>%
filter(n > 50)
evasion_words %>%
count(word, sort = TRUE) %>%
filter(n > 200) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + ggtitle("The Most Common Words in Corpus") + theme_minimal()
evasion_words %>%
count(word, sort = TRUE) %>%
filter(n > 200) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + ggtitle("Most Common Words in Corpus") + theme_minimal()
contributions <- evasion_words %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
group_by(word) %>%
summarize(occurences = n(),
contribution = sum(score))
contributions
contributions %>%
top_n(50, abs(contribution)) %>%
mutate(word = reorder(word, contribution)) %>%
ggplot(aes(word, contribution, fill = contribution > 0)) +
geom_col(show.legend = FALSE) +
coord_flip() + ggtitle('Words with the Most Contributions to Positive/Negative Sentiment Scores') + theme_minimal()
wiki_bigrams <- evasion %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
letters_bigram_counts <- letters_bigrams %>%
count(year, bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ")
wiki_bigrams <- evasion_words %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
evasion_bigram_counts <- letters_bigrams %>%
count(year, bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ")
wiki_bigrams <- evasion_words %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
evasion_bigram_counts <- evasion_bigrams %>%
count(year, bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ")
evasion_words %>%
count(word) %>%
inner_join(get_sentiments("loughran"), by = "word") %>%
group_by(sentiment) %>%
top_n(5, n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
facet_wrap(~ sentiment, scales = "free") +
ggtitle("Frequency of This Word in Buffett's Letters") + theme_minimal()
contributions %>%
top_n(50, abs(contribution)) %>%
mutate(word = reorder(word, contribution)) %>%
ggplot(aes(word, contribution, fill = contribution > 0)) +
geom_col(show.legend = FALSE) +
coord_flip() + ggtitle('Words with the most contributions to positive/negative sentiment scores') + theme_minimal()
evasion_words %>%
count(word) %>%
inner_join(get_sentiments("loughran"), by = "word") %>%
group_by(sentiment) %>%
top_n(5, n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
facet_wrap(~ sentiment, scales = "free") +
ggtitle("Frequency of words in corpus") + theme_minimal()
evasion_words %>%
count(word) %>%
inner_join(get_sentiments("loughran"), by = "word") %>%
group_by(sentiment) %>%
top_n(10, n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
facet_wrap(~ sentiment, scales = "free") +
ggtitle("Frequency of words in corpus by sentiment category") + theme_minimal()
wiki_bigrams <- evasion_words %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
evasion_bigram_counts <- evasion_bigrams %>%
count(year, bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ")
wiki_bigrams <- evasion_words %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
evasion_bigram_counts <- wiki_bigrams %>%
count(year, bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ")
wiki_bigrams <- evasion_words %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
evasion_bigram_counts <- wiki_bigrams %>%
count(bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ")
negate_words <- c("not", "without", "no", "can't", "don't", "won't")
evasion_bigram_counts %>%
filter(word1 %in% negate_words) %>%
count(word1, word2, wt = n, sort = TRUE) %>%
inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
mutate(contribution = score * nn) %>%
group_by(word1) %>%
top_n(10, abs(contribution)) %>%
ungroup() %>%
mutate(word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
ggplot(aes(word2, contribution, fill = contribution > 0)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ word1, scales = "free", nrow = 3) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
xlab("Words followed by a negation") +
ylab("Sentiment score * # of occurrences") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
coord_flip() + ggtitle("Words that contributed the most to sentiment when they followed a â€˜negation'") + theme_minimal()
wiki_bigrams <- evasion_words %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
evasion_bigram_counts <- wiki_bigrams %>%
count(bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
top_n(10, n) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
facet_wrap(~ sentiment, scales = "free") +
ggtitle("Frequency of words in corpus by sentiment category") + theme_minimal()
wiki_bigrams <- evasion_words %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
evasion_bigram_counts <- wiki_bigrams %>%
count(bigram, sort = TRUE) %>%
ungroup() %>%
separate(bigram, c("word1", "word2"), sep = " ")
wiki_bigrams
unnested_words <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)) %>%
mutate(filename = .x) %>%
unnest_tokens(word, txt))
directory <- "/Users/benjaminkinsella/Documents/GitHub/Tax-Fraud-and-NLP-Demo/pdfs"
all_pdfs <- list.files(pattern = ".pdf$")
Corpus <- map_df(all_pdfs, ~ data_frame(txt = pdf_text(.x)
